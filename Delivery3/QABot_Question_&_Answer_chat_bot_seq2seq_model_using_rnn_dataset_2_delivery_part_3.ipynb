{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "QABot_Question_&_Answer_chat_bot_seq2seq_model_using_rnn_dataset_2_delivery_part_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ed59469c3df42ffa5875e2c91b3d40b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_020352f596814a03a267da0ac6f24812",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ad8b5eba15c94724b5cb7b7a1a8abd92",
              "IPY_MODEL_868209aed0834f2e8f5c26763db88aae"
            ]
          }
        },
        "020352f596814a03a267da0ac6f24812": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ad8b5eba15c94724b5cb7b7a1a8abd92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_13e53891f63046e49c7c7f37d6202606",
            "_dom_classes": [],
            "description": "BLEU: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 19350,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 19350,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_322a8c4977fd449c92e0aa921e8c82e3"
          }
        },
        "868209aed0834f2e8f5c26763db88aae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6c344762f2e14b25994048b3325dd2f3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 19350/19350 [24:44&lt;00:00, 13.04it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cde97e9dca9a46e98839157f1d90f991"
          }
        },
        "13e53891f63046e49c7c7f37d6202606": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "322a8c4977fd449c92e0aa921e8c82e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6c344762f2e14b25994048b3325dd2f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cde97e9dca9a46e98839157f1d90f991": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c6c3032469844e979d585ce80746d8fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ee8d5ccb123549bebd3b7649e007aecd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_88ec071d25134a4dac12525092f6327e",
              "IPY_MODEL_f2969ba666cd40b39d8a2639d53eecc4"
            ]
          }
        },
        "ee8d5ccb123549bebd3b7649e007aecd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "88ec071d25134a4dac12525092f6327e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bdc431dfad3c4e02b65413806678684f",
            "_dom_classes": [],
            "description": "F1 Score: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 19350,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 19350,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ec388cedcb5a44f4b0f1b72ab36c4006"
          }
        },
        "f2969ba666cd40b39d8a2639d53eecc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9da666e5606847dfa04549f6fb02477a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 19350/19350 [24:57&lt;00:00, 12.92it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_470bcd97fc0948488eed6c5d8db635ff"
          }
        },
        "bdc431dfad3c4e02b65413806678684f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ec388cedcb5a44f4b0f1b72ab36c4006": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9da666e5606847dfa04549f6fb02477a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "470bcd97fc0948488eed6c5d8db635ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e9820ff2367745e6b769ba525dc21516": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be5938f66e1a47a0aba64bc40bf64560",
              "IPY_MODEL_43d256a08b0841639ce9ff93d8cb02c8"
            ],
            "layout": "IPY_MODEL_589a795cbdb643e28f9bc1656cec627c"
          }
        },
        "be5938f66e1a47a0aba64bc40bf64560": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "ROUGE-L: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83b69d2ae0f54585976e4aec45c8c4fa",
            "max": 19350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ad2ce9f0a844b45bd15dc4b3463127d",
            "value": 19350
          }
        },
        "43d256a08b0841639ce9ff93d8cb02c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d6873b397f44adf957925d1de896ab8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8d817b40331a4fb7ab1479b065b089aa",
            "value": " 19350/19350 [22:26&lt;00:00, 14.37it/s]"
          }
        },
        "589a795cbdb643e28f9bc1656cec627c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83b69d2ae0f54585976e4aec45c8c4fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ad2ce9f0a844b45bd15dc4b3463127d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "1d6873b397f44adf957925d1de896ab8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d817b40331a4fb7ab1479b065b089aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pvd1E4IzMd6b",
        "outputId": "f996efd9-3e8f-4d31-887e-3fe09455c944"
      },
      "source": [
        "!pip install wget\n",
        "!pip install clean-text\n",
        "!pip install torchtext==0.6.0\n",
        "!pip install  sentencepiece  \n",
        "!pip install transformers==2.8.0\n",
        "\n",
        "import re\n",
        "import os\n",
        "import csv\n",
        "import time\n",
        "import math\n",
        "import json\n",
        "import random\n",
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from cleantext import clean\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torchtext import data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel, DistilBertTokenizer, DistilBertModel, GPT2Tokenizer, GPT2LMHeadModel \n",
        "from torchsummary import summary\n",
        "from torchtext.data.metrics import bleu_score\n",
        "from tqdm.autonotebook import tqdm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=c25f5180749786fadf9dc538740b4196d14b2b9872562bcc1381362268a9a646\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Collecting clean-text\n",
            "  Downloading https://files.pythonhosted.org/packages/78/30/7013e9bf37e00ad81406c771e8f5b071c624b8ab27a7984cd9b8434bed4f/clean_text-0.3.0-py3-none-any.whl\n",
            "Collecting ftfy<6.0,>=5.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 4.8MB/s \n",
            "\u001b[?25hCollecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/1c/1f1457fe52d0b30cbeebfd578483cedb3e3619108d2d5a21380dfecf8ffd/emoji-0.6.0.tar.gz (51kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51kB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy<6.0,>=5.8->clean-text) (0.2.5)\n",
            "Building wheels for collected packages: ftfy, emoji\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.8-cp36-none-any.whl size=45612 sha256=d1d22c28a0635375c1f1dbbd729b4686c5af3360fb9fc9c5848789f39589d819\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.6.0-cp36-none-any.whl size=49716 sha256=3977498e45f2ad13ac6946ec2bebf2e0d436a9a61976696f233bff5f23b09e32\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2c/8b/9dcf5216ca68e14e0320e283692dce8ae321cdc01e73e17796\n",
            "Successfully built ftfy emoji\n",
            "Installing collected packages: ftfy, emoji, clean-text\n",
            "Successfully installed clean-text-0.3.0 emoji-0.6.0 ftfy-5.8\n",
            "Collecting torchtext==0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/17/e7c588245aece7aa93f360894179374830daf60d7ed0bbb59332de3b3b61/torchtext-0.6.0-py3-none-any.whl (64kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (4.41.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 13.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed sentencepiece-0.1.94 torchtext-0.6.0\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.94)\n",
            "Collecting transformers==2.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 573kB 11.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2.23.0)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/71/8025cafe9780b6102b9c564b75a0865781e84699d4d2c0d458e5664560b6/boto3-1.16.27.tar.gz (97kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102kB 8.3MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.7MB 26.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 48.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.18.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.1.94)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (4.41.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2020.11.8)\n",
            "Collecting botocore<1.20.0,>=1.19.27\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/e6/f41a1936b7602d9badb66ae9677e5d4cfc8bd9955a9a1618a0945f2f5b1b/botocore-1.19.27-py2.py3-none-any.whl (7.0MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.0MB 37.9MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (0.17.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.27->boto3->transformers==2.8.0) (2.8.1)\n",
            "Building wheels for collected packages: boto3, sacremoses\n",
            "  Building wheel for boto3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for boto3: filename=boto3-1.16.27-py2.py3-none-any.whl size=128454 sha256=d0ae36ea063530c3eeae2d08f6e5281f32cf99fe9301f330d6f35c0229bf51b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/4a/29/e5b74fd7012b8322191f5db368a910a78b013ce96bf4259d50\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=0e7149c2896f0d67879b69ee733aa2748b619f5feda9625bf98474bc9e853898\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built boto3 sacremoses\n",
            "\u001b[31mERROR: botocore 1.19.27 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, tokenizers, sacremoses, transformers\n",
            "Successfully installed boto3-1.16.27 botocore-1.19.27 jmespath-0.10.0 s3transfer-0.3.3 sacremoses-0.0.43 tokenizers-0.5.2 transformers-2.8.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sisACf4KFT2",
        "outputId": "848bb6dd-e8a5-4ca1-fc7b-7ffb6bafcc29"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDZLhT-QNXWd"
      },
      "source": [
        "### **Constants And Required Functions**\n",
        "\n",
        "**Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNQq6kjfMT1Y"
      },
      "source": [
        "\"\"\"\n",
        "Device\n",
        "\"\"\"\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "\"\"\"\n",
        "Batch Size\n",
        "\"\"\"\n",
        "BATCH_SIZE= 6\n",
        "\n",
        "isTrain = False\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seed(915)\n",
        "\n",
        "\n",
        "def textPreprocess(txt):\n",
        "    \"\"\"\n",
        "    Convert to Lowercase and Trim the text\n",
        "    \"\"\"\n",
        "    txt = txt.lower().strip()\n",
        "\n",
        "    \"\"\"\n",
        "    Fix various unicode errors\n",
        "    transliterate to closest ASCII representation\n",
        "    \"\"\"\n",
        "    txt = clean(txt, fix_unicode=True, to_ascii=True)\n",
        "\n",
        "    \"\"\"\n",
        "    Removing zero-width character\n",
        "    \"\"\"\n",
        "    txt = re.sub(u\"\\ufe0f\", r\" \", txt)\n",
        "\n",
        "    \"\"\"\n",
        "    Remove URL\n",
        "    \"\"\"\n",
        "    txt = re.sub(r\"https?://[A-Za-z0-9./]*\", r\" \", txt)\n",
        "\n",
        "    \"\"\"\n",
        "    Remove Specific Special character\n",
        "    \"\"\"\n",
        "    txt = re.sub(r\"[-.!?()_]+\", r\" \", txt)\n",
        "    \"\"\"\n",
        "    Remove charatcter like special characters, punctuations except alphanumeric charatcter.\n",
        "    \"\"\"\n",
        "    txt = re.sub(r\"[^0-9a-zA-Z]+\", r\" \", txt)\n",
        "\n",
        "    \"\"\"\n",
        "    Remove Extra spaces which are appearing from previous processing steps.\n",
        "   \"\"\"\n",
        "    txt = re.sub(r\"\\s+\", r\" \", txt).strip()\n",
        "    return txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d1rTljBEOtH"
      },
      "source": [
        "**Required Directories Creation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Gh0V-1-KK9y",
        "outputId": "ad980144-463d-4601-8f96-d2e6fcc9289e"
      },
      "source": [
        "\"\"\"\n",
        "Required functions for directory creation\n",
        "\"\"\"\n",
        "def check_if_dir_exists(directory):\n",
        "    \"\"\"\n",
        "    Checks if 'directory' exists\n",
        "    \"\"\"\n",
        "    return(os.path.isdir(directory))\n",
        "\n",
        "def make_dir(directory):\n",
        "    \"\"\"\n",
        "    Create directory\n",
        "    \"\"\"\n",
        "    if not check_if_dir_exists(directory):\n",
        "        os.mkdir(directory)\n",
        "        print(\"Directory %s created successfully.\" %directory)\n",
        "    else:\n",
        "        print(\"Directory %s exists.\" %directory)\n",
        "\n",
        "print(\"We are in:\",os.getcwd())\n",
        "\n",
        "\"\"\"\n",
        "Required directory creation\n",
        "\"\"\"\n",
        "chatbot_dir=\"/content/gdrive/My Drive/DATA_SCIENCE_Capstone_Project_ChatBot_Sofia_Dutta\"\n",
        "make_dir(chatbot_dir)\n",
        "\n",
        "os.chdir(\"/content/gdrive/My Drive/DATA_SCIENCE_Capstone_Project_ChatBot_Sofia_Dutta\")\n",
        "\n",
        "chatbot_data_dir = chatbot_dir + \"/ChatBot_Data/\"\n",
        "make_dir(chatbot_data_dir)\n",
        "\n",
        "chatbot_result_dir = chatbot_dir + \"/ChatBot_Results/\"\n",
        "make_dir(chatbot_result_dir)\n",
        "\n",
        "chatbot_checkpoint_dir = chatbot_dir + \"/ChatBot_Checkpoint/\"\n",
        "make_dir(chatbot_checkpoint_dir)\n",
        "\n",
        "print(chatbot_data_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We are in: /content\n",
            "Directory /content/gdrive/My Drive/DATA_SCIENCE_Capstone_Project_ChatBot_Sofia_Dutta exists.\n",
            "Directory /content/gdrive/My Drive/DATA_SCIENCE_Capstone_Project_ChatBot_Sofia_Dutta/ChatBot_Data/ exists.\n",
            "Directory /content/gdrive/My Drive/DATA_SCIENCE_Capstone_Project_ChatBot_Sofia_Dutta/ChatBot_Results/ exists.\n",
            "Directory /content/gdrive/My Drive/DATA_SCIENCE_Capstone_Project_ChatBot_Sofia_Dutta/ChatBot_Checkpoint/ exists.\n",
            "/content/gdrive/My Drive/DATA_SCIENCE_Capstone_Project_ChatBot_Sofia_Dutta/ChatBot_Data/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8RvHvrgEZh5"
      },
      "source": [
        "**Listing Directories**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJJ6EITDdHjD",
        "outputId": "d3c90e63-569b-4cae-f294-1f35d920e1b1"
      },
      "source": [
        "def list_dir(dir_path):\n",
        "  \"\"\"\n",
        "  List directories for a given path\n",
        "  \"\"\"\n",
        "  print(\"Directory %s contains : \" %dir_path)\n",
        "  for dir_or_file in os.listdir(dir_path):\n",
        "    print(dir_or_file)\n",
        "  print(\"\\n\")\n",
        "\n",
        "\"\"\"\n",
        "List created directories\n",
        "\"\"\"\n",
        "print('Current directory : ', os.getcwd(),'\\n')\n",
        "list_dir(chatbot_dir)\n",
        "list_dir(chatbot_data_dir)\n",
        "ist_dir(chatbot_checkpoint_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current directory :  /content/gdrive/My Drive/DATA_SCIENCE_Capstone_Project_ChatBot_Sofia_Dutta \n",
            "\n",
            "Directory /content/gdrive/My Drive/DATA_SCIENCE_Capstone_Project_ChatBot_Sofia_Dutta contains : \n",
            "ChatBot_Data\n",
            "ChatBot_Results\n",
            "ChatBot_Checkpoint\n",
            ".vector_cache\n",
            "Images\n",
            "\n",
            "\n",
            "Directory /content/gdrive/My Drive/DATA_SCIENCE_Capstone_Project_ChatBot_Sofia_Dutta/ChatBot_Data/ contains : \n",
            ".kaggle\n",
            "dev_v2.1.json.gz\n",
            "dev_v2.1.json\n",
            "question_asked.tsv\n",
            "QAWhats.tsv\n",
            "twcs.csv\n",
            "QA_Pair.tsv\n",
            "Apple_QA_Pair.tsv\n",
            "Q_asked\n",
            "question_asked\n",
            "\n",
            "\n",
            "Directory /content/gdrive/My Drive/DATA_SCIENCE_Capstone_Project_ChatBot_Sofia_Dutta/ChatBot_Checkpoint/ contains : \n",
            "QA_Seq2Seq_ChatBot_BestModel.pt\n",
            "QA_Seq2Seq_ChatBot.pt\n",
            "QA_GPT_Seq2seq_ChatBot.pt\n",
            "QA_GPT_Seq2seq_Results.csv\n",
            "Apple_QA_Seq2Seq_ChatBot.pt\n",
            "Apple_QA_Seq2Seq_ChatBot_BestModel.pt\n",
            "Apple_QA_GPT_Seq2seq_ChatBot.pt\n",
            "Apple_QA_GPT_Seq2seq_Results.csv\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-xvJMZFuA2Y"
      },
      "source": [
        "# **Evaluation Matrix For First Model for Chatbot using Seq2seq Approach**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0yoPLBlR55A"
      },
      "source": [
        "### **Dataset and  DataLoader Creation**\n",
        "\n",
        "* For Evaluation Require Test Dataset and  DataLoader Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctO4dzkOR1WN",
        "outputId": "21626b53-3c95-4075-84e9-5eec347b8e2d"
      },
      "source": [
        "\"\"\"\n",
        "Batch Size, Embedding Size, Hidden Size\n",
        "\"\"\"\n",
        "BATCH_SZ, EMBEDDING_SIZE, HIDDEN_SIZE, NUM_LAYERS, EPOCHS = 32, 64, 256, 3, 20\n",
        "\"\"\"\n",
        "Sentence start, end and pad token\n",
        "\"\"\"\n",
        "SOS_TOKEN, EOS_TOKEN, PAD_TOKEN = \"<SOS>\", \"<EOS>\", \"<_PADDING_>\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "QA Pair, word to integer map And integer to word map Creation Function\n",
        "\"\"\"\n",
        "def getVocab(qa_df):\n",
        "     \n",
        "    qa_pairs, vocab2idx, idx2vocab = [], {}, {}\n",
        "    \"\"\"\n",
        "    Question Answer Pair Creation\n",
        "    \"\"\"\n",
        "    qa_pairs=[(qa_df.iloc[idx]['question'],qa_df.iloc[idx]['answer']) for idx in range(len(qa_df))]\n",
        "    print('Number of question and answer pairs) : ',len(qa_pairs))\n",
        "   \n",
        "    \"\"\"\n",
        "    Creation of word to integer map.\n",
        "    \"\"\"\n",
        "    vocab=set(word for question, answer in qa_pairs for sentance in (question, answer) for word in sentance.split(\" \"))\n",
        "    print('Number of vocab : ',len(vocab))\n",
        "\n",
        "    vocab2idx = {w:i for i,w in enumerate(vocab,3)}\n",
        "    vocab2idx[PAD_TOKEN], vocab2idx[SOS_TOKEN], vocab2idx[EOS_TOKEN] = 0, 1, 2\n",
        "    print('Number of keys in vocab2idx : ',len(vocab2idx))\n",
        "\n",
        "    \"\"\"\n",
        "    Creation of integer to word map.\n",
        "    \"\"\"\n",
        "    idx2vocab = {idx:word for word, idx in vocab2idx.items()}\n",
        "    print('Number of keys in idx2vocab : ',len(idx2vocab))\n",
        "      \n",
        "    return qa_pairs, vocab2idx, idx2vocab\n",
        "\n",
        "\"\"\"\n",
        "QA DataSet Creation Function\n",
        "\"\"\"\n",
        "class QADataset(Dataset):\n",
        "\n",
        "    def __init__(self, lang_pairs, vocab2idx):\n",
        "        self.lang_pairs = lang_pairs\n",
        "        self.vocab2idx = vocab2idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lang_pairs)\n",
        "\n",
        "    def addToken(self, txt, flag):\n",
        "        if flag:\n",
        "            return SOS_TOKEN + \" \" + txt + \" \" + EOS_TOKEN\n",
        "        else:\n",
        "            return txt + \" \" + EOS_TOKEN\n",
        "\n",
        "    def getTensor(self, txt):\n",
        "        return torch.tensor([self.vocab2idx[wrd] for wrd in txt.split(\" \")], dtype=torch.int64)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.getTensor(self.addToken(self.lang_pairs[idx][0],True)), self.getTensor(self.addToken(self.lang_pairs[idx][1],False))\n",
        "\n",
        "        return x, y\n",
        "\n",
        "\"\"\"\n",
        "Pad items in the batch to the length of the longest item in the batch\n",
        "\"\"\"\n",
        "def collate(batch):\n",
        "\n",
        "    len_tuples=[(i[0].size(0) , i[1].size(0))  for i in batch]\n",
        "\n",
        "    max_x, max_y = tuple(map(max, zip(*len_tuples)))\n",
        "\n",
        "    getBatch = lambda batch, idx, max_len : torch.stack([F.pad(src_trg[idx], (0,max_len-src_trg[idx].size(0)), value=PAD) for src_trg in batch])\n",
        "    \n",
        "    X,Y = getBatch(batch, 0, max_x), getBatch(batch, 1, max_y)\n",
        "    \n",
        "    return (X, Y), Y\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Load Refined Data\n",
        "\"\"\"\n",
        "qa_final_df=pd.read_csv(chatbot_data_dir+'Apple_QA_Pair.tsv',sep='\\t')\n",
        "print('Shape of qa_final_df : ',qa_final_df.shape)\n",
        "qa_final_df.head()\n",
        "\"\"\"\n",
        "Create qa_pairs, vocab2idx, idx2vocab\n",
        "\"\"\"\n",
        "qa_pairs, vocab2idx, idx2vocab = getVocab(qa_final_df)\n",
        "PAD = vocab2idx[PAD_TOKEN]\n",
        "SOS = vocab2idx[SOS_TOKEN]\n",
        "EOS = vocab2idx[EOS_TOKEN]\n",
        "\n",
        "\"\"\"\n",
        "Create QA DataSet\n",
        "\"\"\"\n",
        "qa_dataset = QADataset(qa_pairs, vocab2idx)\n",
        "\n",
        "\"\"\"\n",
        "Data Split\n",
        "\"\"\"\n",
        "train_size, test_size = round(len(qa_dataset)*0.8),len(qa_dataset)-round(len(qa_dataset)*0.8)\n",
        "\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(qa_dataset, [train_size, test_size])\n",
        "\n",
        "\"\"\"\n",
        "Create DataLoader\n",
        "\"\"\"\n",
        "train_loader = DataLoader(train_dataset, batch_size = BATCH_SZ, shuffle = True, collate_fn = collate)\n",
        "test_loader = DataLoader(test_dataset, batch_size = BATCH_SZ, collate_fn = collate)\n",
        "\n",
        "print(\"\\n\\nHow does the tensor look?\\n\",train_dataset[8])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of qa_final_df :  (96750, 2)\n",
            "Number of question and answer pairs) :  96750\n",
            "Number of vocab :  35075\n",
            "Number of keys in vocab2idx :  35078\n",
            "Number of keys in idx2vocab :  35078\n",
            "\n",
            "\n",
            "How does the tensor look?\n",
            " (tensor([    1,  1530, 31549,  2713, 33282, 22879,  4114,  3806,  9206, 34541,\n",
            "        12057,  3820, 12720, 23014, 12187, 12057,  3475,     2]), tensor([28294, 12187, 20625,  4868,  2639,  3456,  7267, 12859, 12057,  9073,\n",
            "        30162, 23118, 20185, 31533,  2378, 28581, 12070,  4251, 13627,     2]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juk4moMyFPQp"
      },
      "source": [
        "# **Question & Answer ChatBot  QABot Model Creation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e70DFBSGYeYd"
      },
      "source": [
        "## **Attention**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9J3KhLYy5yl"
      },
      "source": [
        "\"\"\"\n",
        "Attention Mechanism Layers\n",
        "\"\"\"\n",
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "    def dot_score(self, hidden_encoded, hidden_decoded):\n",
        "        \"\"\"\n",
        "        *******\n",
        "        Input\n",
        "        *******\n",
        "        hidden_encoded  : (B, T, D) \n",
        "        hidden_decoded  : (B, D)\n",
        "\n",
        "        *******\n",
        "        Output\n",
        "        *******\n",
        "        attention_score :  (B, T, 1)\n",
        "        \"\"\"\n",
        "        return torch.bmm(hidden_encoded, hidden_decoded.unsqueeze(2)) / np.sqrt(hidden_encoded.size(2))\n",
        "        \n",
        "        \n",
        "    def forward(self, hidden_encoded, hidden_decoded, mask=None):\n",
        "        \"\"\"\n",
        "        *******\n",
        "        Input\n",
        "        *******\n",
        "\n",
        "        hidden_encoded   : (B, T, D)\n",
        "        hidden_decoded   : (B, D)\n",
        "        attention_scores : (B, T, 1) \n",
        "        mask             : (B, T) \n",
        "            \n",
        "        *******\n",
        "        Output\n",
        "        *******\n",
        "        context           : (B, D) \n",
        "        attention_weight  : (B, T, 1)\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        Attention scores\n",
        "        \"\"\"\n",
        "        attention_scores = self.dot_score(hidden_encoded, hidden_decoded)\n",
        "\n",
        "        if mask is not None:\n",
        "            attention_scores[~mask] = float(-1000)\n",
        "        \n",
        "        \"\"\"\n",
        "        Attention weight\n",
        "        \"\"\"\n",
        "        attention_weight = F.softmax(attention_scores, dim=1)\n",
        "        \n",
        "        \"\"\"\n",
        "        (B, T, D) * (B, T, 1) to (B, D)\n",
        "        \"\"\"\n",
        "        context = (hidden_encoded * attention_weight).sum(dim=1)\n",
        "        \n",
        "        return context, attention_weight\n",
        "\n",
        "def maskedFill(input, time_dimension=1, fill=0):\n",
        "    \"\"\"\n",
        "    Generate Mask of shape (B, T) to determine input sequence length.\n",
        "    \"\"\"\n",
        "    dimensions = list(range(1,len(input.shape))) \n",
        "    \n",
        "    if time_dimension in dimensions:\n",
        "        dimensions.remove(time_dimension)\n",
        "       \n",
        "    with torch.no_grad():\n",
        "        if len(dimensions) == 0:\n",
        "            return (input != fill)\n",
        "        \n",
        "        mask = torch.sum((input != fill), dim=dimensions) > 0\n",
        "    return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmxxTTtiYio1"
      },
      "source": [
        "## **Encoder**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsUt5vtVlp6H"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, embeddding_size, hidden_size, n_layers=1, bidirectional=True):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "\n",
        "        self.encoder_layer = nn.GRU(input_size = embeddding_size, \n",
        "                                    hidden_size = hidden_size//2, \n",
        "                                    num_layers = n_layers, \n",
        "                                    bidirectional = bidirectional)\n",
        "\n",
        "    def forward(self, question_embd, question_len):\n",
        "        \n",
        "        \"\"\"\n",
        "        Pack the sequences as question sequences are of varying length.\n",
        "        \"\"\"\n",
        "        embed_packed = pack_padded_sequence(question_embd,\n",
        "                                            question_len, \n",
        "                                            batch_first=True,\n",
        "                                            enforce_sorted=False)\n",
        "        \n",
        "        enc_outs, h_enc = self.encoder_layer(embed_packed)\n",
        "        \"\"\"\n",
        "        As bidirectional : (B, T, 2, D//2)\n",
        "        \"\"\"\n",
        "        enc_outs, _ = pad_packed_sequence(enc_outs) \n",
        "        \n",
        "        batch_size, time_step = question_embd.size(0), question_embd.size(1)\n",
        "        \"\"\"\n",
        "        (B, T, 2, D//2) to (B, T, D)\n",
        "        \"\"\"\n",
        "        enc_outs = enc_outs.view(batch_size, time_step , -1) \n",
        "\n",
        "        hidden_size = enc_outs.size(2) \n",
        "\n",
        "        \"\"\"\n",
        "        Reshaping the h_enc as (n_layers, directions, batch_size, hidden_size).\n",
        "        Take the last layer's output.\n",
        "        \"\"\"\n",
        "        h_enc = h_enc.view(-1, 2, batch_size, hidden_size//2)[-1,:,:,:] \n",
        "        \"\"\"\n",
        "        Reordering to (B, 2, D/2) and reshaping to (B, D)\n",
        "        \"\"\"\n",
        "        h_enc = h_enc.permute(1, 0, 2).reshape(batch_size, -1)\n",
        "\n",
        "\n",
        "        return enc_outs, h_enc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVeNzvDpYoKy"
      },
      "source": [
        "## **Decoder**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S291JiSgKT-U"
      },
      "source": [
        "class AttentionDecoderRNN(nn.Module):\n",
        "\n",
        "    def __init__(self,vocab_size, embedding_size, hidden_size, n_layers=1):\n",
        "        super(AttentionDecoderRNN, self).__init__()\n",
        "\n",
        "        \"\"\"\n",
        "        Decoder is uni-directionall and used GRUCells so to do the decoding one step at a time.\n",
        "        \"\"\"\n",
        "        self.decoder_layers = nn.ModuleList([nn.GRUCell(embedding_size, hidden_size)] + \n",
        "                                            [nn.GRUCell(hidden_size, hidden_size) for i in range(n_layers-1)])\n",
        "        \n",
        "        self.attention = Attention()\n",
        "\n",
        "        \"\"\"\n",
        "        Prediction, a fully connected network to convert the attention context and decoded context to a predicted next token\n",
        "        \"\"\"\n",
        "        self.prediction = nn.Sequential(nn.Linear(2*hidden_size, hidden_size),\n",
        "                                        nn.LeakyReLU(),\n",
        "                                        nn.LayerNorm(hidden_size),\n",
        "                                        nn.Linear(hidden_size, hidden_size),\n",
        "                                        nn.LeakyReLU(),\n",
        "                                        nn.LayerNorm(hidden_size),\n",
        "                                        nn.Linear(hidden_size, vocab_size)\n",
        "                                      )\n",
        "\n",
        "    def forward(self, decoder_input, h_previous, encoded_outs, mask):\n",
        "\n",
        "        \n",
        "        for layer in range(len(self.decoder_layers)):  \n",
        "            next_hidden_state = self.decoder_layers[layer](decoder_input, h_previous[layer])\n",
        "            \n",
        "            h_previous[layer], decoder_input = next_hidden_state, next_hidden_state\n",
        "            \n",
        "\n",
        "        \"\"\"\n",
        "        (B, D)\n",
        "        \"\"\"    \n",
        "        answer_decoded = decoder_input \n",
        "\n",
        "        \"\"\"\n",
        "        Attention mechanism, to get relevant information from the previous encoded states.\n",
        "        (B, T, 1)\n",
        "        \"\"\" \n",
        "        attention_context, attention_weights = self.attention(encoded_outs, answer_decoded, mask=mask)\n",
        "        \n",
        "        \"\"\"\n",
        "        Concatinating the attention context and the decoded context.\n",
        "        (B, D) + (B, D)  to (B, 2*D)\n",
        "        \"\"\"\n",
        "        pred_token = torch.cat((attention_context, answer_decoded), dim=1) \n",
        "        \"\"\"\n",
        "        Predict the next token.\n",
        "        (B, 2*D) to (B, V)\n",
        "        \"\"\"\n",
        "        pred_token = self.prediction(pred_token) \n",
        "\n",
        "        return attention_weights, pred_token, h_previous"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkVBj0dNYt0g"
      },
      "source": [
        "## **Seq2Seq**\n",
        "\n",
        "The heart of chatbot is a sequence-to-sequence (seq2seq) model. The goal of a seq2seq model is to take a variable-length question sequence as an input, and return a variable-length answer sequence as an output.\n",
        "\n",
        "***Components :***\n",
        "\n",
        "* I have used `nn.Embedding` layer to convert tokens into feature vectors.\n",
        "\n",
        "* Next, I have used `nn.GRU`, an encoding RNN that takes a tensor of shape $(B, T, D)$, since it expects all $T$ items at once. As the entire question is taken, I used _bidrectional_ `nn.GRU`. \n",
        "\n",
        "* For decoder RNN I have used `nn.GRUCell`, a _uni-derectional_ decoding RNN as it generates the output one item at a time.\n",
        "\n",
        "* To prevent an infinite loop in the case of a bad prediction, I have set a limit of 22 `decode_steps` to control the maximum number of decoding steps.\n",
        "\n",
        "\n",
        "* ***For training*** I used a combination of the auto-regressive and teacher forcing approaches by randomly decidung which approach to take.\n",
        "* ***For prediction*** I used the auto-regressive approach since teacher forcing requires us to know the answer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFsUH40JL5JS"
      },
      "source": [
        "class Seq2SeqAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size, pad_idx=None, n_layers=1, decode_steps=22):\n",
        "        super(Seq2SeqAttention, self).__init__()\n",
        "\n",
        "        self.pad_idx = pad_idx\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=self.pad_idx)\n",
        "        \n",
        "        \n",
        "        self.encoder = EncoderRNN(embedding_size,\n",
        "                                  hidden_size,\n",
        "                                  n_layers)\n",
        "        \n",
        "\n",
        "        self.attndecoder = AttentionDecoderRNN(vocab_size,\n",
        "                                               embedding_size,\n",
        "                                               hidden_size,\n",
        "                                               n_layers)\n",
        "        \n",
        "        self.decode_steps = decode_steps               \n",
        "    \n",
        "    def forward(self, question):\n",
        "        \"\"\"\n",
        "        question either (B, T) or ((B, T), (B, T'))\n",
        "        \"\"\"\n",
        "        if isinstance(question, tuple):\n",
        "            question, answer = question\n",
        "        else:\n",
        "            answer = None\n",
        "\n",
        "        batch_size, time_dimension  = question.size(0), question.size(1)\n",
        "\n",
        "        \"\"\"\n",
        "        Embedding\n",
        "        (B, T) to (B, T, D)\n",
        "        \"\"\"\n",
        "        embeded = self.embedding(question) \n",
        "        device = self.embedding.weight.device\n",
        "        mask = maskedFill(embeded)\n",
        "        question_lengths = mask.sum(dim=1).view(-1) \n",
        "\n",
        "        \"\"\"\n",
        "        Encoding\n",
        "        \"\"\"\n",
        "        encoded_outs, h_encoded = self.encoder(embeded, question_lengths)\n",
        "\n",
        "        \"\"\"\n",
        "        Attention Decoding \n",
        "        encoded_outs : encoded feature vectors of the question data.\n",
        "        h_encoded : the initial input for the decoder.\n",
        "        \"\"\"\n",
        "        h_previous = [h_encoded for _ in range(self.n_layers)]\n",
        "        attention_weights, predictions = [], []\n",
        "        \n",
        "        \"\"\"\n",
        "        Last token of question, EOS marker as the first input for the decoder.\n",
        "        \"\"\"\n",
        "        decoder_input = self.embedding(question[:,-1]) \n",
        "\n",
        "        decode_steps = self.decode_steps\n",
        "        \"\"\"\n",
        "        Training : Given Question and Answer pairs gives exact decode length.\n",
        "        Testing  : Given decode_steps.\n",
        "        \"\"\"\n",
        "        if answer is not None: \n",
        "            decode_steps = answer.size(1)\n",
        "        \n",
        "        \"\"\"\n",
        "        Either Teacher Forcing OR Auto-Regressive\n",
        "        \"\"\"\n",
        "        teacher_forcing = np.random.choice((True,False))\n",
        "        for decode_step in range(decode_steps):\n",
        "            \"\"\"\n",
        "            (B, D)\n",
        "            \"\"\"\n",
        "            decoder_in = decoder_input   \n",
        "\n",
        "            attention_weight, pred_token, h_previous = self.attndecoder(decoder_in, h_previous, encoded_outs, mask)\n",
        "            \n",
        "            attention_weights.append(attention_weight.detach())  \n",
        "            predictions.append(pred_token)\n",
        "            \n",
        "            \"\"\"\n",
        "            Selecting the token for the next time step. \n",
        "            torch.no_grad() : In-order to prevent the gradient to pass through the question tokens.\n",
        "            \"\"\"\n",
        "            with torch.no_grad():\n",
        "                if self.training:\n",
        "                    if answer is not None and teacher_forcing:\n",
        "                        \"\"\"\n",
        "                        Teacher Forcing : next correct token.\n",
        "                        \"\"\"\n",
        "                        next_token = answer[:,decode_step].squeeze()\n",
        "                    else:\n",
        "                        \"\"\"\n",
        "                        Auto-Regressive : next token based on the prediction.\n",
        "                        \"\"\"\n",
        "                        next_token = torch.multinomial(F.softmax(pred_token, dim=1), 1)[:,-1]\n",
        "                else:\n",
        "                    \"\"\"\n",
        "                    For testing : selecting most likely token.\n",
        "                    \"\"\"\n",
        "                    next_token = torch.argmax(pred_token, dim=1)\n",
        "            \n",
        "            \"\"\"\n",
        "            Next token is the decoder input for next time step further time step's token prediction. \n",
        "            \"\"\"\n",
        "            decoder_input = self.embedding(next_token.to(device))\n",
        "        \n",
        "\n",
        "        prediction, attention_score = torch.stack(predictions, dim=1),  torch.stack(attention_weights, dim=1).squeeze()\n",
        "        \n",
        "        return prediction if self.training else prediction, attention_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzEr4YXUjgfi"
      },
      "source": [
        "## **Loading Best Seq2seq ChatBot Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDEDgZ7HjULv"
      },
      "source": [
        "checkpointFile = chatbot_checkpoint_dir +'Apple_QA_Seq2Seq_ChatBot.pt'\n",
        "checkpointFileBestModel = chatbot_checkpoint_dir +'Apple_QA_Seq2Seq_ChatBot_BestModel.pt'\n",
        "\n",
        "seq2seq_model = Seq2SeqAttention(vocab_size      = len(vocab2idx), \n",
        "                                 embedding_size  = EMBEDDING_SIZE, \n",
        "                                 hidden_size     = HIDDEN_SIZE,\n",
        "                                 pad_idx         = PAD, \n",
        "                                 n_layers        = NUM_LAYERS\n",
        "                                 )\n",
        "\"\"\"\n",
        "Gradient Cliping\n",
        "\"\"\"\n",
        "for param in seq2seq_model.parameters():\n",
        "    param.register_hook(lambda grad: torch.clamp(grad, -10, 10))\n",
        "\n",
        "\"\"\"\n",
        "Load State Dict of Best Model\n",
        "\"\"\"\n",
        "checkpoint_dict = torch.load(checkpointFileBestModel)\n",
        "seq2seq_model.load_state_dict(checkpoint_dict['model_state_dict'])\n",
        "\n",
        "seq2seq_model = seq2seq_model.eval().cpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe8ma1CuZHZ4"
      },
      "source": [
        "# **Evaluation : Test Results**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFi75rVmU3Sk"
      },
      "source": [
        "getWords = lambda x : [idx2vocab[idx] for idx in x.cpu().numpy()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ayqahzl8ZV8Z"
      },
      "source": [
        "## **BLEU Score**\n",
        "\n",
        "\n",
        "* From `test_dataset` getting `question_tensor`, `answer_tensor`.\n",
        "\n",
        "* Passing `question_tensor` to seq2seq model in eval mode and with `torch.no_grad()` to prevent gradient updation. This gives predicted answer tensor.\n",
        "\n",
        "* Converting predicted answer tensor and `answer_tensor` to string using idx to vocabulary mapping.\n",
        "\n",
        "* Computing the BLEU score between a candidate answer and a predicted answer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "0ed59469c3df42ffa5875e2c91b3d40b",
            "020352f596814a03a267da0ac6f24812",
            "ad8b5eba15c94724b5cb7b7a1a8abd92",
            "868209aed0834f2e8f5c26763db88aae",
            "13e53891f63046e49c7c7f37d6202606",
            "322a8c4977fd449c92e0aa921e8c82e3",
            "6c344762f2e14b25994048b3325dd2f3",
            "cde97e9dca9a46e98839157f1d90f991"
          ]
        },
        "id": "T6dma8kmXgT6",
        "outputId": "2e557d9c-0c00-48e3-a2ed-e5e8e80de5ce"
      },
      "source": [
        "\"\"\"\n",
        "Function that calculate BLEU Score\n",
        "\"\"\"\n",
        "def calculateBleuScore(model):\n",
        "    \n",
        "    answers, pred_answers = [], []\n",
        "\n",
        "    for idx in tqdm(range(len(test_dataset)), desc=\"BLEU\", disable=False):\n",
        "      \n",
        "        question_tensor, answer_tensor = test_dataset[idx]\n",
        "  \n",
        "        with torch.no_grad():\n",
        "            predictions, attn_score = model(question_tensor.unsqueeze(0))\n",
        "            pred = torch.argmax(predictions, dim=2)\n",
        "        \n",
        "        ans_words, pred_ans = getWords(answer_tensor), getWords(pred[0,:])\n",
        "\n",
        "        answers.append([ans_words[:-1]])\n",
        "        pred_answers.append(pred_ans)\n",
        "    \n",
        "    return bleu_score(pred_answers, answers)\n",
        "\n",
        "\"\"\"\n",
        "Calculate BLEU Score\n",
        "\"\"\"\n",
        "bleu = calculateBleuScore(seq2seq_model)\n",
        "print('BLEU Score : {:.4f}'.format(bleu))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ed59469c3df42ffa5875e2c91b3d40b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='BLEU', max=19350.0, style=ProgressStyle(description_widthâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "BLEU Score : 0.4861\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRMdZdCfihRj"
      },
      "source": [
        "# **F1 Score**\n",
        "\n",
        "* From `test_dataset` getting `question_tensor`, `answer_tensor`.\n",
        "\n",
        "* Passing `question_tensor` to seq2seq model in eval mode and with `torch.no_grad()` to prevent gradient updation. This gives predicted answer tensor.\n",
        "\n",
        "* Converting predicted answer tensor and `answer_tensor` to string using idx to vocabulary mapping.\n",
        "\n",
        "* Count the number of common words between them.\n",
        "\n",
        "* Calculate Precesion and Recall.\n",
        "\n",
        "* Calculate F1 Score based on the following formula.\n",
        "\n",
        "* F1 Score takes into account cooccurring words regardless their orders.\n",
        "\n",
        "* **F1 Score :**   \n",
        "\n",
        "$$\n",
        "\\frac{2 \\times precession \\times recall}{precession + recall}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "c6c3032469844e979d585ce80746d8fa",
            "ee8d5ccb123549bebd3b7649e007aecd",
            "88ec071d25134a4dac12525092f6327e",
            "f2969ba666cd40b39d8a2639d53eecc4",
            "bdc431dfad3c4e02b65413806678684f",
            "ec388cedcb5a44f4b0f1b72ab36c4006",
            "9da666e5606847dfa04549f6fb02477a",
            "470bcd97fc0948488eed6c5d8db635ff"
          ]
        },
        "id": "Q92QSyw_jerb",
        "outputId": "35d4ef26-cc72-4310-e54e-244fd32beb88"
      },
      "source": [
        "def calculate_f1_score(model):\n",
        "    f1_scores = []\n",
        "    for idx in tqdm(range(len(test_dataset)), desc=\"F1 Score\", disable=False):\n",
        "        question_tensor, answer_tensor = test_dataset[idx]\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            predictions, attention_score = model(question_tensor.unsqueeze(0))\n",
        "            pred = torch.argmax(predictions, dim=2)\n",
        "\n",
        "        answer_words, pred_ans = getWords(answer_tensor), getWords(pred[0,:])\n",
        "\n",
        "        number_of_common_words = sum((collections.Counter(answer_words) & collections.Counter(pred_ans)).values())\n",
        "\n",
        "        if number_of_common_words == 0:\n",
        "            f1_score =  0\n",
        "        else:\n",
        "            precision = 1.0 * number_of_common_words / len(pred_ans)\n",
        "            recall = 1.0 * number_of_common_words / len(answer_words)\n",
        "            f1_score = (2 * precision * recall) / (precision + recall)\n",
        "            \n",
        "        f1_scores.append(f1_score)\n",
        "    return f1_scores\n",
        "\n",
        "f1_scores = calculate_f1_score(seq2seq_model)\n",
        "f1_score =(sum(f1_scores)/len(f1_scores))\n",
        "print('F1 Score : {:.4f}'.format(f1_score))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6c3032469844e979d585ce80746d8fa",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='F1 Score', max=19350.0, style=ProgressStyle(description_wâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "F1 Score : 0.9053\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bob0AZ20ik9w"
      },
      "source": [
        "# **Rouge L Score**\n",
        "\n",
        "[RougeL](https://www.aclweb.org/anthology/W04-1013.pdf)\n",
        "\n",
        "> \"Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and\n",
        "word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. \n",
        "\n",
        "> \"Given two sequences X and Y, the longest common subsequence (LCS) of X and recall reflects the proportion of words in X (reference summary sentence) that are also present in Y (candidate summary sentence); while unigram precision is the proportion of words in Y that are also in X. Unigram recall and precision count all cooccurring words regardless their orders; while\n",
        "ROUGE-L counts only in-sequence co-occurrences.\"\n",
        "\n",
        "ROUGE-L is one type of ROUGE measures. It is calculated by taking into account longest common subsequence (LCS) between two sequences.It counts only in-sequence co-occurrences.\n",
        "\n",
        "* From `test_dataset` getting `question_tensor`, `answer_tensor`.\n",
        "\n",
        "* Passing `question_tensor` to seq2seq model in eval mode and with `torch.no_grad()` to prevent gradient updation. This gives predicted answer tensor.\n",
        "\n",
        "* Converting predicted answer tensor and `answer_tensor` to string using idx to vocabulary mapping.\n",
        "\n",
        "* Computing the Rouge-L score between a candidate answer and a predicted answer by getting longest common subsequence (LCS) between the two sequences.\n",
        "\n",
        "* **Applying ROUGE-L Score Formula :** \n",
        "\n",
        "$$\n",
        "\\frac{(1+\\beta^2) \\times R \\times P}{R + \\beta^2 \\times P}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "e9820ff2367745e6b769ba525dc21516",
            "be5938f66e1a47a0aba64bc40bf64560",
            "43d256a08b0841639ce9ff93d8cb02c8",
            "589a795cbdb643e28f9bc1656cec627c",
            "83b69d2ae0f54585976e4aec45c8c4fa",
            "9ad2ce9f0a844b45bd15dc4b3463127d",
            "1d6873b397f44adf957925d1de896ab8",
            "8d817b40331a4fb7ab1479b065b089aa"
          ]
        },
        "id": "RRLuzSGxva_Y",
        "outputId": "28e44227-587b-454d-c3e2-c3a52e347fde"
      },
      "source": [
        "def longest_common_subsequence(str1, str2):\n",
        "        \n",
        "    \"\"\"\n",
        "    Makeing a grid of 0's with len(str2) + 1 columns  and len(str1) + 1 rows.\n",
        "    \"\"\"\n",
        "    dp = [[0] * (len(str2) + 1) for _ in range(len(str1) + 1)]\n",
        "    \n",
        "    \"\"\"\n",
        "    Iterate up each column, starting from the last one.\n",
        "    \"\"\"\n",
        "    for col in reversed(range(len(str2))):\n",
        "        for row in reversed(range(len(str1))):\n",
        "            \n",
        "            if str2[col] == str1[row]:\n",
        "                \"\"\"\n",
        "                If the corresponding characters for this cell are the same.\n",
        "                \"\"\"\n",
        "                dp[row][col] = 1 + dp[row + 1][col + 1]\n",
        "            \n",
        "            else:\n",
        "                \"\"\"\n",
        "                Otherwise they must be different.\n",
        "                \"\"\"\n",
        "                dp[row][col] = max(dp[row + 1][col], dp[row][col + 1])\n",
        "    \n",
        "    \"\"\"\n",
        "    The original problem's answer is in dp[0][0]. Return it.\n",
        "    \"\"\"\n",
        "    return dp[0][0]\n",
        "    \n",
        "def rougel_score(ans, pred):\n",
        "    \n",
        "    BETA, answers, pred_answers = 1.2, [], []\n",
        "        \n",
        "    if len(pred)!=1 and len(ans)<=0:\n",
        "        return        \n",
        "    \n",
        "    for idx in range(min(len(pred),len(ans))):\n",
        "        pred_words, ans_words = pred[idx], ans[idx]\n",
        "        long_cmmn_subseq = longest_common_subsequence(ans_words, pred_words)\n",
        "        answers.append(long_cmmn_subseq/float(len(ans_words)))\n",
        "        pred_answers.append(long_cmmn_subseq/float(len(pred_words)))\n",
        "        \n",
        "    max_ans, max_pred = max(answers), max(pred_answers)\n",
        "    \n",
        "    \"\"\"\n",
        "    Rouge-L Score\n",
        "    \"\"\"\n",
        "    return ((1 + BETA**2)* max_pred * max_ans)/float(max_ans + BETA**2 * max_pred) if (max_ans !=0 and max_pred !=0) else 0.0\n",
        "\n",
        "def calculateRougeLScore(model):\n",
        "    \n",
        "    answers, pred_answers = [], []\n",
        "\n",
        "    for idx in tqdm(range(len(test_dataset)), desc=\"ROUGE-L\", disable=False):\n",
        "      \n",
        "        question_tensor, answer_tensor = test_dataset[idx]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions, attn_score = model(question_tensor.unsqueeze(0))\n",
        "            pred = torch.argmax(predictions, dim=2)\n",
        "        \n",
        "        answer_words, pred_ans = getWords(answer_tensor), getWords(pred[0,:])\n",
        "\n",
        "        pred_answers.append(pred_ans)\n",
        "        answers.append(answer_words[:-1])\n",
        "\n",
        "    return rougel_score(answers, pred_answers)\n",
        "\n",
        "rouge_l_score = calculateRougeLScore(seq2seq_model)\n",
        "print('ROUGE-L Score : {:.4f}'.format(rouge_l_score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e9820ff2367745e6b769ba525dc21516",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='ROUGE-L', max=19350.0, style=ProgressStyle(description_wiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "ROUGE-L Score : 0.9606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e7InqwQF2_p"
      },
      "source": [
        "##**Chat**\n",
        "\n",
        "***Start Conversation with the Bot***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "PBQ1n-3uvnmg",
        "outputId": "ece19aa2-9e97-4f10-965d-f53c11c690bc"
      },
      "source": [
        "def bot_response(question):\n",
        "\n",
        "    question = SOS_TOKEN + \" \" + textPreprocess(question) + \" \" + EOS_TOKEN\n",
        "    \n",
        "    question_tensor = torch.tensor([vocab2idx[w] for w in question.split(\" \")], dtype=torch.int64)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        predictions, attention_score = seq2seq_model(question_tensor.unsqueeze(0))\n",
        "        pred = torch.argmax(predictions, dim=2)\n",
        " \n",
        "    pred_words = getWords(pred[0,:]) \n",
        "    return \" \".join([w for w in pred_words if not (w == '<EOS>')])\n",
        "\n",
        "\n",
        "question = ''\n",
        "print('Bot : Hi, Did you want to chat with me?')\n",
        "while question.lower()[:3] != 'bye':\n",
        "  try:\n",
        "    while True:\n",
        "        print('Me : ', end='')\n",
        "        question = input()\n",
        "        if question:\n",
        "            break\n",
        "    if question.lower()[:3] != 'bye':\n",
        "        response = bot_response(question)\n",
        "        print('Bot: ' + response)\n",
        "    else:\n",
        "        print('Bot: Bye!! Stay safe. Have a nice day.')\n",
        "  except KeyError:\n",
        "            print(\"Sorry, I am not sure what you are talking about :/\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bot : Hi, Did you want to chat with me?\n",
            "Me : My last os update is not working.\n",
            "Bot: we'd like to help  dm us the details of the issues you're experiencing and we'll go from there  <url>\n",
            "Me : This needs to be fixed as my music randomly pauses.\n",
            "Bot: we want to help  which iphone and ios version are you using        \n",
            "Me : iphone and ios version is 11.\n",
            "Bot: thanks for letting us know  let's continue in dm  <url>         \n",
            "Me : Also after the update,unable to connect to wifi automatically.\n",
            "Bot: we want to help  which device are you using           \n",
            "Me : iphone.\n",
            "Bot: thanks for that info  let's continue in dm  <url>          \n",
            "Me : Thanks for your help!!!\n",
            "Bot: you're welcome  we're glad to hear  reach out to us if you need any more help  have a great\n",
            "Me : Bye!!!\n",
            "Bot: Bye!! Stay safe. Have a nice day.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}